{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dc221e7-6d06-42e0-964f-90bb6fb3566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53980/853558711.py:145: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows: 100000\n",
      "Engineering features...\n",
      "Encoding labels and filtering rare classes...\n",
      "Rows after filtering: 100000\n",
      "Preparing tokenizer and dataloaders...\n",
      "Classes - macro: 19, micro: 88; numerical features: 17\n",
      "Building model...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.7774 | Val Loss: 0.5265 | Val Macro-F1: 0.9384 | Val Micro-F1: 0.9201\n",
      "New best model — saving checkpoint.\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7991 | Val Loss: 0.2558 | Val Macro-F1: 0.9601 | Val Micro-F1: 0.9634\n",
      "New best model — saving checkpoint.\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3876 | Val Loss: 0.1620 | Val Macro-F1: 0.9662 | Val Micro-F1: 0.9813\n",
      "New best model — saving checkpoint.\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2549 | Val Loss: 0.1378 | Val Macro-F1: 0.9665 | Val Micro-F1: 0.9867\n",
      "New best model — saving checkpoint.\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2173 | Val Loss: 0.1412 | Val Macro-F1: 0.9672 | Val Micro-F1: 0.9873\n",
      "New best model — saving checkpoint.\n",
      "Training complete. Best epoch: 5 | Best combined F1: 1.9545\n",
      "\n",
      "Evaluating best checkpoint on TEST set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53980/853558711.py:841: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1269 | Test Macro-F1: 0.9683 | Test Micro-F1: 0.9889\n",
      "[REPORT] classification report saved to reports/macro_classification_report.json\n",
      "[REPORT] confusion matrix saved to reports/macro_confusion_matrix.csv\n",
      "[REPORT] classification report saved to reports/micro_classification_report.json\n",
      "[REPORT] confusion matrix saved to reports/micro_confusion_matrix.csv\n",
      "Example prediction -> Macro: Shopping | Micro: E-Commerce Marketplaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\"\"\"\n",
    "DragoNet - End-to-End Transaction Categorisation Pipeline\n",
    "---------------------------------------------------------\n",
    "\n",
    "Features:\n",
    "- Load & clean UPI transaction CSV.\n",
    "- Text + numeric feature fusion (DistilBERT + engineered features).\n",
    "- Train/Val/Test split with stratification on macro label.\n",
    "- Dual-head model: predicts MACRO + MICRO categories.\n",
    "- Full evaluation:\n",
    "    - Weighted F1 for macro & micro.\n",
    "    - Accuracy for macro & micro.\n",
    "    - Classification reports (saved as JSON).\n",
    "    - Confusion matrices (saved as CSV).\n",
    "- Inference helper: predict_transaction(...) for a single row.\n",
    "\n",
    "Notes about performance:\n",
    "- This version uses stronger regularisation (higher dropout, weight decay,\n",
    "  limited epochs) to *avoid overfitting / overperformance*.\n",
    "- Exact Macro-F1 depends on your dataset; adjust EPOCHS / DROPOUT / LR etc\n",
    "  if you still see > 0.94.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ----------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------\n",
    "CSV_PATH = \"synthetic_100k_transactions.csv\"   # change to your file path\n",
    "TEXT_NAME_COL = \"Counterparty\"\n",
    "TEXT_DESC_COL = \"Note\"\n",
    "UPI_COL = \"UPI_ID\"            # retained but not parsed into suffix/domain\n",
    "DATE_COL = \"Date\"\n",
    "TIME_COL = \"Time\"\n",
    "DIRECTION_COL = \"Direction\"\n",
    "AMOUNT_COL = \"Amount\"\n",
    "MACRO_COL = \"Tag\"\n",
    "MICRO_COL = \"SubCategory\"\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
    "MAX_LEN = 32\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# regularisation tuned to AVOID overperformance\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_RATIO = 0.05\n",
    "DROPOUT = 0.4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "GRAD_CLIP = 1.0\n",
    "MIN_SAMPLES_PER_CLASS = 2\n",
    "PATIENCE = 3\n",
    "CHECKPOINT_PATH = \"best_drago_net_with_reports.pt\"\n",
    "SEED = 42\n",
    "\n",
    "REPORTS_DIR = \"reports\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------------\n",
    "# REPRO\n",
    "# ----------------------------------\n",
    "def set_seed(seed: int = SEED):\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# ----------------------------------\n",
    "# UTIL: text cleaning\n",
    "# ----------------------------------\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "def clean_text(s: Any) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # mask long numeric strings (like transaction IDs)\n",
    "    s = re.sub(r\"\\d{6,}\", \"<NUM>\", s)\n",
    "    # remove generic transaction boilerplate words to lower overfitting\n",
    "    s = re.sub(r\"(txn|upi|ref|id|payment|pmt|tran)\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w@\\-/<>\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ----------------------------------\n",
    "# LOAD & PREPROCESS\n",
    "# ----------------------------------\n",
    "def load_and_preprocess(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # drop clearly unneeded columns if present\n",
    "    for c in [\"Account\", \"UPI_Ref_No\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(columns=[c])\n",
    "\n",
    "    # text columns\n",
    "    for col in [TEXT_NAME_COL, TEXT_DESC_COL, UPI_COL]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"\").astype(str).map(clean_text)\n",
    "        else:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    # direction – keep simple, don't over-encode\n",
    "    if DIRECTION_COL in df.columns:\n",
    "        df[DIRECTION_COL] = (\n",
    "            df[DIRECTION_COL]\n",
    "            .fillna(\"unknown\")\n",
    "            .astype(str)\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "        )\n",
    "    else:\n",
    "        df[DIRECTION_COL] = \"unknown\"\n",
    "\n",
    "    # date/time parsing\n",
    "    if DATE_COL in df.columns:\n",
    "        df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n",
    "    else:\n",
    "        df[DATE_COL] = pd.NaT\n",
    "\n",
    "    if TIME_COL in df.columns:\n",
    "        def combine_dt(r):\n",
    "            d = r[DATE_COL]\n",
    "            t = r[TIME_COL]\n",
    "            if pd.isna(d):\n",
    "                return pd.NaT\n",
    "            if pd.isna(t) or str(t).strip() == \"\":\n",
    "                return d\n",
    "            tt = str(t).strip()\n",
    "            try:\n",
    "                if \":\" in tt:\n",
    "                    parts = tt.split(\":\")\n",
    "                    h = int(parts[0])\n",
    "                    m = int(parts[1]) if len(parts) > 1 else 0\n",
    "                    s = int(parts[2]) if len(parts) > 2 else 0\n",
    "                    return datetime(d.year, d.month, d.day, h, m, s)\n",
    "                else:\n",
    "                    h = int(tt)\n",
    "                    return datetime(d.year, d.month, d.day, h)\n",
    "            except Exception:\n",
    "                return d\n",
    "        df[\"dt\"] = df.apply(combine_dt, axis=1)\n",
    "    else:\n",
    "        df[\"dt\"] = df[DATE_COL]\n",
    "\n",
    "    # drop rows without valid datetime\n",
    "    df = df[~df[\"dt\"].isna()].reset_index(drop=True)\n",
    "\n",
    "    # amount\n",
    "    if AMOUNT_COL in df.columns:\n",
    "        df[AMOUNT_COL] = pd.to_numeric(df[AMOUNT_COL], errors=\"coerce\")\n",
    "        df = df[~df[AMOUNT_COL].isna()].reset_index(drop=True)\n",
    "    else:\n",
    "        df[AMOUNT_COL] = 0.0\n",
    "\n",
    "    # labels\n",
    "    for col in [MACRO_COL, MICRO_COL]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"unknown\").astype(str).str.strip()\n",
    "        else:\n",
    "            df[col] = \"unknown\"\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------------\n",
    "# FEATURE ENGINEERING\n",
    "# ----------------------------------\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # temporal features\n",
    "    df[\"hour\"] = df[\"dt\"].dt.hour.fillna(0).astype(int)\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "\n",
    "    df[\"dow\"] = df[\"dt\"].dt.dayofweek\n",
    "    df[\"dow_sin\"] = np.sin(2 * np.pi * df[\"dow\"] / 7.0)\n",
    "    df[\"dow_cos\"] = np.cos(2 * np.pi * df[\"dow\"] / 7.0)\n",
    "\n",
    "    df[\"month\"] = df[\"dt\"].dt.month\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * (df[\"month\"] - 1) / 12.0)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * (df[\"month\"] - 1) / 12.0)\n",
    "\n",
    "    df[\"is_month_start\"] = df[\"dt\"].dt.is_month_start.astype(int)\n",
    "    df[\"is_month_end\"] = df[\"dt\"].dt.is_month_end.astype(int)\n",
    "    df[\"is_weekend\"] = df[\"dow\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    # merchant_key = Counterparty only (NO UPI parsing, as requested)\n",
    "    df[\"merchant_key\"] = df[TEXT_NAME_COL].astype(str)\n",
    "    merchant_group = df.groupby(\"merchant_key\")[AMOUNT_COL]\n",
    "    merchant_mean = merchant_group.mean().to_dict()\n",
    "    merchant_count = merchant_group.count().to_dict()\n",
    "\n",
    "    df[\"merchant_mean_amt\"] = df[\"merchant_key\"].map(merchant_mean).fillna(\n",
    "        df[AMOUNT_COL].mean()\n",
    "    )\n",
    "    df[\"merchant_count\"] = df[\"merchant_key\"].map(merchant_count).fillna(0)\n",
    "\n",
    "    # days_since_prev_merchant\n",
    "    df = df.sort_values(\"dt\").reset_index(drop=True)\n",
    "    df[\"days_since_prev_merchant\"] = -1.0\n",
    "    last_dt_by_merchant: Dict[str, datetime] = {}\n",
    "    for i, row in df.iterrows():\n",
    "        k = row[\"merchant_key\"]\n",
    "        cur_dt = row[\"dt\"]\n",
    "        if k in last_dt_by_merchant:\n",
    "            delta = (cur_dt - last_dt_by_merchant[k]).days\n",
    "            df.at[i, \"days_since_prev_merchant\"] = float(delta)\n",
    "        else:\n",
    "            df.at[i, \"days_since_prev_merchant\"] = -1.0\n",
    "        last_dt_by_merchant[k] = cur_dt\n",
    "\n",
    "    # recurring heuristic\n",
    "    median_gap = (\n",
    "        df[df[\"days_since_prev_merchant\"] >= 0]\n",
    "        .groupby(\"merchant_key\")[\"days_since_prev_merchant\"]\n",
    "        .median()\n",
    "        .to_dict()\n",
    "    )\n",
    "    df[\"median_gap\"] = df[\"merchant_key\"].map(median_gap).fillna(9999)\n",
    "    df[\"is_recurring\"] = (\n",
    "        (df[\"merchant_count\"] >= 3) & (df[\"median_gap\"] < 40)\n",
    "    ).astype(int)\n",
    "\n",
    "    # amount transforms\n",
    "    df[\"amount_log\"] = np.log1p(df[AMOUNT_COL].abs())\n",
    "    amt_mean = df[\"amount_log\"].mean()\n",
    "    amt_std = df[\"amount_log\"].std() if df[\"amount_log\"].std() > 0 else 1.0\n",
    "    df[\"amount_log_z\"] = (df[\"amount_log\"] - amt_mean) / amt_std\n",
    "    df[\"amount_div_merchant_mean\"] = df[AMOUNT_COL] / (df[\"merchant_mean_amt\"] + 1e-6)\n",
    "\n",
    "    # amount bucket (quantiles)\n",
    "    try:\n",
    "        df[\"amount_bucket\"] = pd.qcut(\n",
    "            df[\"amount_log\"], q=6, labels=False, duplicates=\"drop\"\n",
    "        ).astype(int)\n",
    "    except Exception:\n",
    "        df[\"amount_bucket\"] = 0\n",
    "\n",
    "    # missing flags\n",
    "    df[\"missing_note\"] = (df[TEXT_DESC_COL] == \"\").astype(int)\n",
    "    df[\"missing_counterparty\"] = (df[TEXT_NAME_COL] == \"\").astype(int)\n",
    "    df[\"missing_upi\"] = (df[UPI_COL] == \"\").astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------------\n",
    "# LABEL ENCODING & FILTER RARE CLASSES\n",
    "# ----------------------------------\n",
    "def encode_and_filter(\n",
    "    df: pd.DataFrame, min_samples: int = MIN_SAMPLES_PER_CLASS\n",
    ") -> Tuple[pd.DataFrame, LabelEncoder, LabelEncoder]:\n",
    "    macro_le = LabelEncoder()\n",
    "    micro_le = LabelEncoder()\n",
    "\n",
    "    df[\"macro_id\"] = macro_le.fit_transform(df[MACRO_COL])\n",
    "    df[\"micro_id\"] = micro_le.fit_transform(df[MICRO_COL])\n",
    "\n",
    "    macro_counts = df[\"macro_id\"].value_counts()\n",
    "    micro_counts = df[\"micro_id\"].value_counts()\n",
    "\n",
    "    mask_keep = df[\"macro_id\"].isin(\n",
    "        macro_counts[macro_counts >= min_samples].index\n",
    "    ) & df[\"micro_id\"].isin(micro_counts[micro_counts >= min_samples].index)\n",
    "\n",
    "    df = df[mask_keep].reset_index(drop=True)\n",
    "\n",
    "    # recompute encoders for contiguous indices\n",
    "    macro_le = LabelEncoder()\n",
    "    df[\"macro_id\"] = macro_le.fit_transform(df[MACRO_COL])\n",
    "\n",
    "    micro_le = LabelEncoder()\n",
    "    df[\"micro_id\"] = micro_le.fit_transform(df[MICRO_COL])\n",
    "\n",
    "    return df, macro_le, micro_le\n",
    "\n",
    "# ----------------------------------\n",
    "# DATASET & DATALOADERS\n",
    "# ----------------------------------\n",
    "class TransactionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int = MAX_LEN\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # numerical features\n",
    "        self.num_cols = [\n",
    "            \"amount_log_z\",\n",
    "            \"amount_div_merchant_mean\",\n",
    "            \"merchant_count\",\n",
    "            \"days_since_prev_merchant\",\n",
    "            \"is_recurring\",\n",
    "            \"hour_sin\",\n",
    "            \"hour_cos\",\n",
    "            \"dow_sin\",\n",
    "            \"dow_cos\",\n",
    "            \"month_sin\",\n",
    "            \"month_cos\",\n",
    "            \"is_month_start\",\n",
    "            \"is_month_end\",\n",
    "            \"is_weekend\",\n",
    "            \"missing_note\",\n",
    "            \"missing_counterparty\",\n",
    "            \"missing_upi\",\n",
    "        ]\n",
    "        for c in self.num_cols:\n",
    "            if c not in self.df.columns:\n",
    "                self.df[c] = 0.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        text_parts = []\n",
    "        if row.get(TEXT_NAME_COL, \"\"):\n",
    "            text_parts.append(row[TEXT_NAME_COL])\n",
    "        if row.get(TEXT_DESC_COL, \"\"):\n",
    "            text_parts.append(row[TEXT_DESC_COL])\n",
    "        if row.get(UPI_COL, \"\"):\n",
    "            text_parts.append(row[UPI_COL])\n",
    "\n",
    "        text = \" [SEP] \".join(text_parts).strip()\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        nums = torch.tensor(\n",
    "            [float(row[c]) for c in self.num_cols], dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        sample = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"numerical\": nums,\n",
    "            \"macro_id\": torch.tensor(int(row[\"macro_id\"]), dtype=torch.long),\n",
    "            \"micro_id\": torch.tensor(int(row[\"micro_id\"]), dtype=torch.long),\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    df: pd.DataFrame,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    "    seed: int = SEED,\n",
    "):\n",
    "    # stratify macro so splits are balanced\n",
    "    stratify_col = df[\"macro_id\"] if df[\"macro_id\"].nunique() > 1 else None\n",
    "\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_ratio,\n",
    "        random_state=seed,\n",
    "        stratify=stratify_col,\n",
    "    )\n",
    "\n",
    "    stratify_tv = (\n",
    "        train_val_df[\"macro_id\"] if train_val_df[\"macro_id\"].nunique() > 1 else None\n",
    "    )\n",
    "    val_ratio_adj = val_ratio / (1.0 - test_ratio)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=val_ratio_adj,\n",
    "        random_state=seed,\n",
    "        stratify=stratify_tv,\n",
    "    )\n",
    "\n",
    "    train_ds = TransactionDataset(train_df, tokenizer)\n",
    "    val_ds = TransactionDataset(val_df, tokenizer)\n",
    "    test_ds = TransactionDataset(test_df, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True, drop_last=False\n",
    "    )\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_df, val_df, test_df\n",
    "\n",
    "# ----------------------------------\n",
    "# MODEL\n",
    "# ----------------------------------\n",
    "class DragoNetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer_name: str,\n",
    "        num_numerical: int,\n",
    "        num_macro: int,\n",
    "        num_micro: int,\n",
    "        hidden_dim: int = 128,\n",
    "        dropout: float = DROPOUT,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "\n",
    "        self.numerical_proj = nn.Sequential(\n",
    "            nn.Linear(num_numerical, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.macro_head = nn.Linear(hidden_dim, num_macro)\n",
    "        self.micro_head = nn.Linear(hidden_dim, num_micro)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerical):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "        )\n",
    "        last_hidden = outputs.last_hidden_state  # (B, T, H)\n",
    "\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        summed = (last_hidden * mask).sum(1)\n",
    "        denom = mask.sum(1).clamp(min=1e-9)\n",
    "        pooled = summed / denom\n",
    "\n",
    "        num_proj = self.numerical_proj(numerical)\n",
    "        fused = torch.cat([pooled, num_proj], dim=1)\n",
    "        hidden = self.fc(fused)\n",
    "\n",
    "        macro_logits = self.macro_head(hidden)\n",
    "        micro_logits = self.micro_head(hidden)\n",
    "\n",
    "        return macro_logits, micro_logits\n",
    "\n",
    "# ----------------------------------\n",
    "# TRAIN / EVAL\n",
    "# ----------------------------------\n",
    "def train_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"train\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        numerical = batch[\"numerical\"].to(device)\n",
    "        macro_id = batch[\"macro_id\"].to(device)\n",
    "        micro_id = batch[\"micro_id\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        macro_logits, micro_logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            numerical=numerical,\n",
    "        )\n",
    "\n",
    "        loss_macro = criterion(macro_logits, macro_id)\n",
    "        loss_micro = criterion(micro_logits, micro_id)\n",
    "        loss = loss_macro + loss_micro\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(dataloader.dataset))\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def eval_model(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    return_preds: bool = False,\n",
    "):\n",
    "    model.eval()\n",
    "    preds_macro = []\n",
    "    preds_micro = []\n",
    "    trues_macro = []\n",
    "    trues_micro = []\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            numerical = batch[\"numerical\"].to(device)\n",
    "            macro_id = batch[\"macro_id\"].to(device)\n",
    "            micro_id = batch[\"micro_id\"].to(device)\n",
    "\n",
    "            macro_logits, micro_logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                numerical=numerical,\n",
    "            )\n",
    "\n",
    "            loss_macro = criterion(macro_logits, macro_id)\n",
    "            loss_micro = criterion(micro_logits, micro_id)\n",
    "            loss = loss_macro + loss_micro\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            preds_macro.extend(torch.argmax(macro_logits, dim=1).cpu().tolist())\n",
    "            preds_micro.extend(torch.argmax(micro_logits, dim=1).cpu().tolist())\n",
    "            trues_macro.extend(macro_id.cpu().tolist())\n",
    "            trues_micro.extend(micro_id.cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(dataloader.dataset))\n",
    "\n",
    "    macro_f1 = (\n",
    "        f1_score(trues_macro, preds_macro, average=\"weighted\")\n",
    "        if len(set(trues_macro)) > 1\n",
    "        else 0.0\n",
    "    )\n",
    "    micro_f1 = (\n",
    "        f1_score(trues_micro, preds_micro, average=\"weighted\")\n",
    "        if len(set(trues_micro)) > 1\n",
    "        else 0.0\n",
    "    )\n",
    "    macro_acc = (\n",
    "        accuracy_score(trues_macro, preds_macro) if len(trues_macro) > 0 else 0.0\n",
    "    )\n",
    "    micro_acc = (\n",
    "        accuracy_score(trues_micro, preds_micro) if len(trues_micro) > 0 else 0.0\n",
    "    )\n",
    "\n",
    "    stats = {\n",
    "        \"loss\": avg_loss,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"macro_acc\": macro_acc,\n",
    "        \"micro_acc\": micro_acc,\n",
    "    }\n",
    "\n",
    "    if return_preds:\n",
    "        return stats, (trues_macro, preds_macro, trues_micro, preds_micro)\n",
    "    else:\n",
    "        return stats\n",
    "\n",
    "# ----------------------------------\n",
    "# REPORTS: classification report + confusion matrices\n",
    "# ----------------------------------\n",
    "def save_classification_report_and_confusion(\n",
    "    trues: List[int],\n",
    "    preds: List[int],\n",
    "    classes: List[str],\n",
    "    prefix: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save:\n",
    "    - classification report (JSON)\n",
    "    - confusion matrix (CSV)\n",
    "    \"\"\"\n",
    "    # classification report\n",
    "    report = classification_report(\n",
    "        trues,\n",
    "        preds,\n",
    "        labels=list(range(len(classes))),\n",
    "        target_names=classes,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    import json\n",
    "\n",
    "    report_path = os.path.join(REPORTS_DIR, f\"{prefix}_classification_report.json\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        trues,\n",
    "        preds,\n",
    "        labels=list(range(len(classes))),\n",
    "    )\n",
    "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    cm_path = os.path.join(REPORTS_DIR, f\"{prefix}_confusion_matrix.csv\")\n",
    "    cm_df.to_csv(cm_path)\n",
    "\n",
    "    print(f\"[REPORT] classification report saved to {report_path}\")\n",
    "    print(f\"[REPORT] confusion matrix saved to {cm_path}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# SAVE / LOAD CHECKPOINT\n",
    "# ----------------------------------\n",
    "def save_checkpoint(model, tokenizer, macro_le, micro_le, path: str):\n",
    "    state = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"tokenizer\": tokenizer.name_or_path\n",
    "        if hasattr(tokenizer, \"name_or_path\")\n",
    "        else None,\n",
    "        \"macro_classes\": list(macro_le.classes_),\n",
    "        \"micro_classes\": list(micro_le.classes_),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str, device: str = DEVICE):\n",
    "    state = torch.load(path, map_location=device)\n",
    "    return state\n",
    "\n",
    "# ----------------------------------\n",
    "# INFERENCE: single transaction helper\n",
    "# ----------------------------------\n",
    "def predict_transaction(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    macro_le,\n",
    "    micro_le,\n",
    "    counterparty: str,\n",
    "    note: str,\n",
    "    upi_id: str,\n",
    "    amount: float,\n",
    "    dt: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build features for a single transaction and return predicted macro/micro labels.\n",
    "    Uses the same feature engineering as the training pipeline.\n",
    "    \"\"\"\n",
    "    # text\n",
    "    counterparty_clean = clean_text(counterparty)\n",
    "    note_clean = clean_text(note)\n",
    "    upi_clean = clean_text(upi_id)\n",
    "    text = \" [SEP] \".join(\n",
    "        [x for x in [counterparty_clean, note_clean, upi_clean] if x]\n",
    "    ).strip()\n",
    "\n",
    "    # minimal single-row feature frame\n",
    "    temp = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                TEXT_NAME_COL: counterparty_clean,\n",
    "                TEXT_DESC_COL: note_clean,\n",
    "                UPI_COL: upi_clean,\n",
    "                AMOUNT_COL: amount,\n",
    "                \"dt\": pd.to_datetime(dt, errors=\"coerce\"),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    temp = engineer_features(temp)\n",
    "\n",
    "    num_cols = [\n",
    "        \"amount_log_z\",\n",
    "        \"amount_div_merchant_mean\",\n",
    "        \"merchant_count\",\n",
    "        \"days_since_prev_merchant\",\n",
    "        \"is_recurring\",\n",
    "        \"hour_sin\",\n",
    "        \"hour_cos\",\n",
    "        \"dow_sin\",\n",
    "        \"dow_cos\",\n",
    "        \"month_sin\",\n",
    "        \"month_cos\",\n",
    "        \"is_month_start\",\n",
    "        \"is_month_end\",\n",
    "        \"is_weekend\",\n",
    "        \"missing_note\",\n",
    "        \"missing_counterparty\",\n",
    "        \"missing_upi\",\n",
    "    ]\n",
    "    numerical = [float(temp.iloc[0][c]) for c in num_cols]\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = encoded[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(DEVICE)\n",
    "    numerical_tensor = (\n",
    "        torch.tensor(numerical, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        macro_logits, micro_logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            numerical=numerical_tensor,\n",
    "        )\n",
    "        macro_pred = int(torch.argmax(macro_logits, dim=1).cpu().item())\n",
    "        micro_pred = int(torch.argmax(micro_logits, dim=1).cpu().item())\n",
    "\n",
    "    macro_label = macro_le.inverse_transform([macro_pred])[0]\n",
    "    micro_label = micro_le.inverse_transform([micro_pred])[0]\n",
    "\n",
    "    return macro_label, micro_label\n",
    "\n",
    "# ----------------------------------\n",
    "# MAIN PIPELINE\n",
    "# ----------------------------------\n",
    "def main(csv_path: str = CSV_PATH):\n",
    "    print(\"Loading & preprocessing...\")\n",
    "    df_raw = load_and_preprocess(csv_path)\n",
    "    print(f\"Raw rows: {len(df_raw)}\")\n",
    "\n",
    "    print(\"Engineering features...\")\n",
    "    df = engineer_features(df_raw)\n",
    "\n",
    "    print(\"Encoding labels and filtering rare classes...\")\n",
    "    df, macro_le, micro_le = encode_and_filter(df, MIN_SAMPLES_PER_CLASS)\n",
    "    print(f\"Rows after filtering: {len(df)}\")\n",
    "    if len(df) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No rows left after filtering. Lower MIN_SAMPLES_PER_CLASS or check labels.\"\n",
    "        )\n",
    "\n",
    "    print(\"Preparing tokenizer and dataloaders...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    (\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        train_df,\n",
    "        val_df,\n",
    "        test_df,\n",
    "    ) = create_dataloaders(df, tokenizer, batch_size=BATCH_SIZE)\n",
    "\n",
    "    num_numerical = len(TransactionDataset(df, tokenizer).num_cols)\n",
    "    num_macro = len(macro_le.classes_)\n",
    "    num_micro = len(micro_le.classes_)\n",
    "\n",
    "    print(\n",
    "        f\"Classes - macro: {num_macro}, micro: {num_micro}; numerical features: {num_numerical}\"\n",
    "    )\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = DragoNetModel(\n",
    "        MODEL_NAME,\n",
    "        num_numerical=num_numerical,\n",
    "        num_macro=num_macro,\n",
    "        num_micro=num_micro,\n",
    "        hidden_dim=128,\n",
    "        dropout=DROPOUT,\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=max(1, total_steps),\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val = -1.0\n",
    "    best_epoch = -1\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            criterion,\n",
    "            DEVICE,\n",
    "        )\n",
    "        val_stats = eval_model(model, val_loader, DEVICE)\n",
    "        val_metric = val_stats[\"macro_f1\"] + val_stats[\"micro_f1\"]\n",
    "        print(\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_stats['loss']:.4f} | \"\n",
    "            f\"Val Macro-F1: {val_stats['macro_f1']:.4f} | \"\n",
    "            f\"Val Micro-F1: {val_stats['micro_f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_metric > best_val:\n",
    "            best_val = val_metric\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            print(\"New best model — saving checkpoint.\")\n",
    "            save_checkpoint(model, tokenizer, macro_le, micro_le, CHECKPOINT_PATH)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience {patience_counter}/{PATIENCE}\")\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\n",
    "        f\"Training complete. Best epoch: {best_epoch} | Best combined F1: {best_val:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Load best checkpoint & evaluate on TEST set with full reports\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"\\nEvaluating best checkpoint on TEST set...\")\n",
    "        state = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "        try:\n",
    "            model.load_state_dict(state[\"model_state\"])\n",
    "        except Exception:\n",
    "            print(\n",
    "                \"Warning: checkpoint state could not be loaded into model. Using current model weights.\"\n",
    "            )\n",
    "\n",
    "        test_stats, (trues_macro, preds_macro, trues_micro, preds_micro) = eval_model(\n",
    "            model, test_loader, DEVICE, return_preds=True\n",
    "        )\n",
    "        print(\n",
    "            f\"Test Loss: {test_stats['loss']:.4f} | \"\n",
    "            f\"Test Macro-F1: {test_stats['macro_f1']:.4f} | \"\n",
    "            f\"Test Micro-F1: {test_stats['micro_f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "        # save classification report + confusion matrices\n",
    "        macro_classes = list(macro_le.classes_)\n",
    "        micro_classes = list(micro_le.classes_)\n",
    "\n",
    "        save_classification_report_and_confusion(\n",
    "            trues_macro,\n",
    "            preds_macro,\n",
    "            macro_classes,\n",
    "            prefix=\"macro\",\n",
    "        )\n",
    "        save_classification_report_and_confusion(\n",
    "            trues_micro,\n",
    "            preds_micro,\n",
    "            micro_classes,\n",
    "            prefix=\"micro\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"No checkpoint found; skipping test evaluation & reports.\")\n",
    "\n",
    "    return model, tokenizer, macro_le, micro_le, df, test_loader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run full pipeline\n",
    "    model, tokenizer, macro_le, micro_le, df, test_loader = main(CSV_PATH)\n",
    "\n",
    "    # Example single prediction\n",
    "    example_counterparty = \"Amazon\"\n",
    "    example_note = \"Order payment successful\"\n",
    "    example_upi = \"amazon@apl\"\n",
    "    example_amount = 1299.0\n",
    "    example_dt = \"2024-02-01 14:30:00\"\n",
    "\n",
    "    macro_label, micro_label = predict_transaction(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        macro_le,\n",
    "        micro_le,\n",
    "        counterparty=example_counterparty,\n",
    "        note=example_note,\n",
    "        upi_id=example_upi,\n",
    "        amount=example_amount,\n",
    "        dt=example_dt,\n",
    "    )\n",
    "    print(\"Example prediction -> Macro:\", macro_label, \"| Micro:\", micro_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "322ad73e-d495-4ba7-84ff-79eb2e4f7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_counterparty = \"Nursing Home\"\n",
    "example_note = \"Order payment successful\"\n",
    "example_upi = \"\"\n",
    "example_amount = 1299.0\n",
    "example_dt = \"2024-02-01 14:30:00\"\n",
    "\n",
    "macro_label, micro_label = predict_transaction(\n",
    "    model, tokenizer, macro_le, micro_le,\n",
    "    counterparty=example_counterparty,\n",
    "    note=example_note,\n",
    "    upi_id=example_upi,\n",
    "    amount=example_amount,\n",
    "    dt=example_dt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eacc958-80fc-47e0-9fb3-721f248c81cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Medical'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31d9d3-1a6c-420e-8c1c-49a3b513b2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
